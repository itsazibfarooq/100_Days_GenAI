{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, List, Type, Tuple, Dict\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.axes._axes import Axes\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as D\n",
    "from tqdm import trange\n",
    "from einops import einsum\n",
    "from torch.func import vmap, jacrev\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampleable(ABC):\n",
    "    @abstractmethod\n",
    "    def sample(self, n_samples: int):\n",
    "        pass \n",
    "    \n",
    "class Simulator(ABC):\n",
    "    @abstractmethod \n",
    "    def step(self, x, h, t):\n",
    "        pass \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def simulate_with_trajectory(self, x0, ts):\n",
    "        # ts: [nts, bs, 1]\n",
    "        # x0: [bs, 2]\n",
    "        tjs = [x0.clone()] \n",
    "        for idx in range(1, ts.shape[0]):\n",
    "            t = ts[idx - 1, :] # [bs, 1]\n",
    "            h = ts[idx, :] - t # [bs, 1]\n",
    "            x0 = self.step(x0, h, t) # [bs, 2]\n",
    "            tjs.append(x0.clone())\n",
    "        return torch.stack(tjs, dim=1) # [bs,nts,2]\n",
    "\n",
    "class ODE(ABC):\n",
    "    @abstractmethod \n",
    "    def drift_term(self, x, t):\n",
    "        pass \n",
    "\n",
    "class EulerSampler(Simulator):\n",
    "    def __init__(self, ode: ODE):\n",
    "        self.ode = ode \n",
    "    \n",
    "    def step(self, x, h, t):\n",
    "        return x + self.ode.drift_term(x, t) * h\n",
    "\n",
    "class SDE(ABC):\n",
    "    @abstractmethod\n",
    "    def drift_term(self, x, t):\n",
    "        pass \n",
    "    \n",
    "    @abstractmethod \n",
    "    def diff_term(self, x, t):\n",
    "        pass \n",
    "\n",
    "class EulerMarySampler(Simulator):\n",
    "    def __init__(self, sde: SDE):\n",
    "        self.sde = sde \n",
    "        \n",
    "    def step(self, x, h, t):\n",
    "        return x + self.sde.drift_term(x, t) * h + self.sde.diff_term(x, t) * torch.sqrt(h) * torch.randn_like(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alpha(ABC):\n",
    "    def __init__(self):\n",
    "        assert torch.allclose(\n",
    "            self(torch.zeros(1,1)), torch.zeros(1,1)\n",
    "        )\n",
    "        assert torch.allclose(\n",
    "            self(torch.ones(1,1)), torch.ones(1,1)\n",
    "        )\n",
    "    @abstractmethod\n",
    "    def __call__(self, t):\n",
    "        pass \n",
    "    \n",
    "    def dt(self, t):\n",
    "        t = t.unsqueeze(-1) # the last dimension must be 1 for broadcasting \n",
    "        return vmap(jacrev(t)).view(-1, 1)\n",
    "\n",
    "class Beta(ABC):\n",
    "    def __init__(self):\n",
    "        assert torch.allclose(\n",
    "            self(torch.zeros(1,1)), torch.ones(1,1)\n",
    "        )\n",
    "        assert torch.allclose(\n",
    "            self(torch.ones(1,1)), torch.zeros(1,1)\n",
    "        )\n",
    "    @abstractmethod \n",
    "    def __call__(self, t):\n",
    "        pass \n",
    "    \n",
    "    def dt(self, t):\n",
    "        t = t.unsqueeze(-1)\n",
    "        return vmap(jacrev(self))(t).view(-1, 1)\n",
    "\n",
    "class LinearAlpha(Alpha):\n",
    "    def __call__(self, t):\n",
    "        return t\n",
    "    \n",
    "    def dt(self, t):\n",
    "        return torch.ones_like(t)\n",
    "\n",
    "class LinearBeta(Beta):\n",
    "    def __call__(self, t):\n",
    "        return 1 - t\n",
    "\n",
    "    def dt(self, t):\n",
    "        return -torch.ones_like(t)\n",
    "\n",
    "class SqrtBeta(Beta):\n",
    "    def __call__(self, t):\n",
    "        return torch.sqrt(1 - t)\n",
    "    \n",
    "    def dt(self, t):\n",
    "        return - 0.5 / (torch.sqrt(1 - t) + 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPP(nn.Module, ABC):\n",
    "    def __init__(self, p_simple: Sampleable, p_data: Sampleable):\n",
    "        super().__init__()\n",
    "        self.p_simple = p_simple\n",
    "        self.p_data = p_data\n",
    "\n",
    "    def sample_marginal_path(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        sample randomly from the data distribution \n",
    "        sample from the path between data point z and starting point \n",
    "        \"\"\"\n",
    "        num_samples = t.shape[0]\n",
    "        z, _ = self.sample_conditioning_variable(num_samples)  \n",
    "        x = self.sample_conditional_path(z, t) \n",
    "        return x\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample_conditioning_variable(self, num_samples: int) -> [torch.Tensor, Optional[torch.Tensor]]: \n",
    "        \"\"\"\n",
    "        sample (z, y) from the join distribution of data and label p(z, y)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def sample_conditional_path(self, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        sample from the path between data point z and initial distribution data point P(.|z)\n",
    "        \"\"\"\n",
    "        pass \n",
    "        \n",
    "    @abstractmethod\n",
    "    def conditional_vector_field(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        the path which is being followed by the conditional probability path u(x|z)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def conditional_score(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute: derivative(log(p(x|z)))\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsotropicGaussian(nn.Module, Sampleable):\n",
    "    def __init__(self, shape: torch.Tensor, std: float):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "        self.std = std \n",
    "    \n",
    "    def sample(self, n_samples: int):\n",
    "        return std * torch.randn_like(n_samples, *self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTSampler(nn.Module, Sampleable):\n",
    "    def __init__(self):\n",
    "        self.dataset = datasets.MNIST(\n",
    "            root='../labs/data',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize((32, 32)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,), (0.5,)),\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "    def sample(self, n_samples):\n",
    "        indices = torch.randperm(len(self.dataset))[:n_samples]\n",
    "        samples, label = zip(*[self.dataset[i] for i in indices])\n",
    "        samples, label = torch.stack(samples), torch.tensor(label, dtype=torch.int64)\n",
    "        return samples, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCPP(CPP):\n",
    "    def __init__(self, p_init: Sampleable, p_data: Sampleable, alpha: LinearAlpha, beta: LinearBeta):\n",
    "        super().__init__(p_init, p_data)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "    def sample_conditioning_variable(self, n_sample: int) -> torch.Tensor:\n",
    "        return self.p_data.sample(n_sample)\n",
    "\n",
    "    def sample_conditional_path(self, z, t):\n",
    "        # sampling from standard gaussian randn() and then changing \n",
    "        # standard deviation and mean of that to represent sampling \n",
    "        # from isotropic gaussian \n",
    "        return self.alpha(t) * z + self.beta(t) * torch.randn_like(z)\n",
    "\n",
    "    def conditional_vector_field(self, x, z, t):\n",
    "        alpha_t = self.alpha(t)\n",
    "        alpha_dt = self.alpha.dt(t)\n",
    "        beta_t = self.beta(t)\n",
    "        beta_dt = self.beta.dt(t)\n",
    "        return (alpha_dt - beta_dt/beta_t * alpha_t) * z + (beta_dt / beta_t) * x\n",
    "\n",
    "    def conditional_score(self, x, z, t):\n",
    "        return (self.alpha(t) * z - x) / self.beta(t) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNISTSampler()\n",
    "samples, labels = mnist.sample(10)\n",
    "samples.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = GCPP(\n",
    "    p_init = IsotropicGaussian(shape=(1,32,32), std=1.0),\n",
    "    p_data = MNISTSampler(),\n",
    "    alpha = LinearAlpha(), \n",
    "    beta = LinearBeta()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, labels = path.sample_conditioning_variable(10)\n",
    "samples.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 3\n",
    "n_samples = 9\n",
    "n_ts = 5\n",
    "\n",
    "ts = torch.linspace(0, 1, n_ts)\n",
    "z, _ = path.sample_conditioning_variable(n_samples)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_ts, figsize=(6 * n_rows * n_ts, 6 * n_rows))\n",
    "for idx in range(ts.shape[0]):\n",
    "    t = ts[idx] # [1]\n",
    "    t = t.view(-1, 1, 1, 1).repeat(n_samples, 1, 1, 1) # [n_samples, 1, 1, 1]\n",
    "    xt = path.sample_conditional_path(z, t) # z:[n_samples, 1, 32, 32], t: [n_samples, 1, 1, 1] xt: [n_samples, 1, 32, 32]\n",
    "    grid = make_grid(xt, nrow=n_rows, normalize=True, value_range=(-1,1)) # grid: [3, 3, 1, 32, 32]\n",
    "    axes[idx].imshow(grid.permute(1, 2, 0).cpu(), cmap=\"gray\") # grid: [3, 3, 32, 32, 1]\n",
    "    axes[idx].axis(\"off\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = ts[4].view(-1, 1, 1, 1).repeat(10000, 1, 1, 1) # [10000, 1, 1, 1]\n",
    "xt = path.sample_marginal_path(tx) # [10000, 1, 32, 32]\n",
    "xt.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tensor):\n",
    "    return (tensor - 0.5) / 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca() \n",
    "ax.imshow(normalize(xt[5125].permute(1,2,0)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedVF(nn.Module, ABC):\n",
    "    @abstractmethod \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, y: torch.Tensor):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFGVF(ODE):\n",
    "    def __init__(self, net: GuidedVF, gs: float):\n",
    "        self.net = net \n",
    "        self.gs = gs \n",
    "    \n",
    "    def drift_term(self, x, t, y):\n",
    "        guided_vf = self.net(x, t, y)\n",
    "        unguided_y = torch.ones_like(y) * 10 \n",
    "        unguided_vf = self.net(x, t, unguided_y)\n",
    "        return (1 - gs) * unguided_vf + gs * guided_vf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(ABC):\n",
    "    def __init__(self, net: GuidedVF):\n",
    "        self.net = net \n",
    "    \n",
    "    @abstractmethod \n",
    "    def get_train_loss(self, batch_size: int):\n",
    "        pass \n",
    "\n",
    "    def model_size(self):\n",
    "        size = 0\n",
    "        for param in self.net.parameters():\n",
    "            size += param.nelement() * param.element_size()\n",
    "        for buffer in self.net.buffers():\n",
    "            size += buffer.nelement() * buffer.element_size()\n",
    "        return size / (1024**2)\n",
    "        \n",
    "    def get_optimizer(self, lr: float):\n",
    "        return torch.optim.Adam(self.net.parameters(), lr=lr)\n",
    "    \n",
    "    def train(self, n_epochs: int, batch_size: int, lr: float = 1e-3):\n",
    "        print(f'Training model of size {self.model_size()}')\n",
    "        self.net.train()\n",
    "        opt = self.get_optimizer(lr)\n",
    "        \n",
    "        for ep in trange(n_epochs):\n",
    "            opt.zero_grad()\n",
    "            loss = self.get_train_loss(batch_size)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        self.net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFGTrainer(Trainer):\n",
    "    def __init__(self, path: GCPP, net: GuidedVF, eta: float):\n",
    "        super().__init__(net)\n",
    "        self.path = path \n",
    "        self.eta = eta \n",
    "    \n",
    "    def get_train_loss(self, batch_size: int):\n",
    "        # Step 1: Sample z,y from p_data\n",
    "        z, y = self.path.p_data.sample(batch_size) # z:[bs, 1, 32, 32]\n",
    "        \n",
    "        # Step 2: Set each label to 10 (i.e., null) with probability eta\n",
    "        probs = torch.rand(y.shape)\n",
    "        y[probs < self.eta] = 10.0\n",
    "        \n",
    "        # Step 3: Sample t and x\n",
    "        ts = torch.rand(batch_size, 1, 1, 1) #ts:[bs, 1, 1, 1]\n",
    "        x = self.path.sample_conditional_path(z, ts) # x: [bs, 1, 32, 32]\n",
    "\n",
    "        # Step 4: Regress and output loss\n",
    "        guided_vf = self.net(x, ts, y) # [bs, c, h, w] \n",
    "        ref_vf = path.conditional_vector_field(x, z, ts) # [bs, c, h, w]\n",
    "        \n",
    "        error = einsum(torch.square(guided_vf - ref_vf), 'b c h w -> b') # [bs,]\n",
    "        return torch.mean(error) # [1]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    timestamp is 1D data and we make it a high dimensional data\n",
    "    so we are representing the 1D timestamp data higher dimension via four transform\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        assert dim % 2 == 0\n",
    "        self.half = dim // 2 # the fourier consist of sin and cos, the total lenght will be d (d/2 sin, d/2 cos)-terms\n",
    "        self.w = nn.Parameter(torch.randn(1, self.half))\n",
    "        \n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        variance of sin/cos is 1/2, to make it unit variance we should multiply it with sqrt(2), as per following rule\n",
    "        var(f(x)) = p\n",
    "        var(a*f(x)) = p*a^2\n",
    "        \"\"\"\n",
    "        t = t.view(-1, 1)\n",
    "        freqs = 2 * torch.pi * self.w * t\n",
    "        sins = torch.sin(freqs)\n",
    "        cos = torch.cos(freqs)\n",
    "        return torch.cat([sins, cos], dim=-1) * torch.sqrt(torch.Tensor([2])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(40, 1)\n",
    "layer = FourierEncoder(40)\n",
    "\n",
    "layer(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, n_channels: int, t_emb_dim: int, y_emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm2d(n_channels),\n",
    "            nn.Conv2d(n_channels, n_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm2d(n_channels),\n",
    "            nn.Conv2d(n_channels, n_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.time_adaptor = nn.Sequential(\n",
    "            nn.Linear(t_emb_dim, t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(t_emb_dim, n_channels)\n",
    "        )\n",
    "        self.y_adaptor = nn.Sequential(\n",
    "            nn.Linear(y_emb_dim, y_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(y_emb_dim, n_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t_emb: torch.Tensor, y_emb: torch.Tensor):\n",
    "        orig = x.clone()\n",
    "        x = self.block1(x)\n",
    "        \n",
    "        t_emb = self.time_adaptor(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        x = x + t_emb\n",
    "        \n",
    "        y_emb = self.time_adaptor(y_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        x = x + y_emb\n",
    "        \n",
    "        x = self.block2(x)\n",
    "        \n",
    "        return orig + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channel: int, out_channel :int, n_res_layers: int, t_emb_dim: int, y_emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.res_block = nn.ModuleList([\n",
    "            Residual(in_channel, t_emb_dim, y_emb_dim) for _ in range(n_res_layers)\n",
    "        ])\n",
    "        self.downsample = nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x, t, y):\n",
    "        for layers in self.res_block:\n",
    "            x = layers(x, t, y)\n",
    "        x = self.downsample(x)\n",
    "        return x\n",
    "    \n",
    "class Midcoder(nn.Module):\n",
    "    def __init__(self, n_channels: int, n_res_layers: int, t_emb_dim: int, y_emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.res_block = nn.ModuleList([\n",
    "            Residual(n_channels, t_emb_dim, y_emb_dim) for _ in range(n_res_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, y: torch.Tensor):\n",
    "        for layer in self.res_block:\n",
    "            x = layer(x, t, y)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channel: int, out_channel: int, n_res_layers: int, t_emb_dim: int, y_emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.res_block = nn.ModuleList([\n",
    "            Residual(out_channel, t_emb_dim, y_emb_dim) for _ in range(n_res_layers)\n",
    "        ])\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, y: torch.Tensor):\n",
    "        x = self.upsample(x)\n",
    "        for layer in self.res_block:\n",
    "            x = layer(x, t, y)\n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTUNet(GuidedVF):\n",
    "    def __init__(self, channels: List[int], n_residual_layers: int, t_emb_dim: int, y_emb_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.init_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, channels[0], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels[0]),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.time_encoder = FourierEncoder(t_emb_dim)\n",
    "        self.label_embed = nn.Embedding(num_embeddings=11, embedding_dim=y_emb_dim)\n",
    "        \n",
    "        encoders = []\n",
    "        decoders = []\n",
    "        \n",
    "        for curr_c, next_c in zip(channels[:-1], channels[1:]):\n",
    "            encoders.append(Encoder(curr_c, next_c, n_residual_layers, t_emb_dim, y_emb_dim))\n",
    "            decoders.append(Decoder(next_c, curr_c, n_residual_layers, t_emb_dim, y_emb_dim))    \n",
    "\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        self.decoders = nn.ModuleList(reversed(decoders))\n",
    "        \n",
    "        self.midcoder = Midcoder(channels[-1], n_residual_layers, t_emb_dim, y_emb_dim)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(channels[0], 1, kernel_size=3, padding=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, t, y):\n",
    "        \n",
    "        x = self.init_conv(x)\n",
    "        t = self.time_encoder(t)\n",
    "        y = self.label_embed(y)\n",
    "        \n",
    "        residual = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, t, y)\n",
    "            residual.append(x.clone())\n",
    "        \n",
    "        x = self.midcoder(x, t, y)\n",
    "        \n",
    "        for decoder in self.decoders:\n",
    "            res = residual.pop()\n",
    "            x  = x + res \n",
    "            x = decoder(x, t, y)\n",
    "        \n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = GCPP(\n",
    "    p_init = IsotropicGaussian(shape=(1,32,32), std=1.0),\n",
    "    p_data = MNISTSampler(),\n",
    "    alpha = LinearAlpha(), \n",
    "    beta = LinearBeta()\n",
    ")\n",
    "\n",
    "unet = MNISTUNet(\n",
    "    channels=[32, 64, 128], \n",
    "    n_residual_layers=2, \n",
    "    t_emb_dim=40, \n",
    "    y_emb_dim=40\n",
    ")\n",
    "\n",
    "trainer = CFGTrainer(path=path, net=unet, eta=0.1)\n",
    "trainer.model_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(n_epochs=5000, lr=1e-3, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with these!\n",
    "samples_per_class = 10\n",
    "num_timesteps = 100\n",
    "guidance_scales = [1.0, 3.0, 5.0]\n",
    "\n",
    "# Graph\n",
    "fig, axes = plt.subplots(1, len(guidance_scales), figsize=(10 * len(guidance_scales), 10))\n",
    "\n",
    "for idx, w in enumerate(guidance_scales):\n",
    "    # Setup ode and simulator\n",
    "    ode = CFGVectorFieldODE(unet, guidance_scale=w)\n",
    "    simulator = EulerSimulator(ode)\n",
    "\n",
    "    # Sample initial conditions\n",
    "    y = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=torch.int64).repeat_interleave(samples_per_class).to(device)\n",
    "    num_samples = y.shape[0]\n",
    "    x0, _ = path.p_simple.sample(num_samples) # (num_samples, 1, 32, 32)\n",
    "\n",
    "    # Simulate\n",
    "    ts = torch.linspace(0,1,num_timesteps).view(1, -1, 1, 1, 1).expand(num_samples, -1, 1, 1, 1).to(device)\n",
    "    x1 = simulator.simulate(x0, ts, y=y)\n",
    "\n",
    "    # Plot\n",
    "    grid = make_grid(x1, nrow=samples_per_class, normalize=True, value_range=(-1,1))\n",
    "    axes[idx].imshow(grid.permute(1, 2, 0).cpu(), cmap=\"gray\")\n",
    "    axes[idx].axis(\"off\")\n",
    "    axes[idx].set_title(f\"Guidance: $w={w:.1f}$\", fontsize=25)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
