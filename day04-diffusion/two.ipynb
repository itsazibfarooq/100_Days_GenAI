{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import torch \n",
    "import torch.distributions as D\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Type, Optional, List\n",
    "import matplotlib.cm as cm\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from matplotlib.axes._axes import Axes\n",
    "import numpy as np\n",
    "from torch.func import vmap, jacrev\n",
    "\n",
    "from sklearn.datasets import make_moons, make_circles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampleable(ABC):\n",
    "    @abstractmethod\n",
    "    def sample(self, n_samples: int):\n",
    "        pass \n",
    "\n",
    "class Density(ABC):\n",
    "    @abstractmethod \n",
    "    def log_density(self, x: torch.Tensor):\n",
    "        pass \n",
    "\n",
    "class Simulator(ABC):\n",
    "    @abstractmethod \n",
    "    def step(self, x, h, t):\n",
    "        pass \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def simulate_with_trajectory(self, x0, ts):\n",
    "        # ts: [nts, bs, 1]\n",
    "        # x0: [bs, 2]\n",
    "        tjs = [x0.clone()] \n",
    "        for idx in range(1, ts.shape[0]):\n",
    "            t = ts[idx - 1, :] # [bs, 1]\n",
    "            h = ts[idx, :] - t # [bs, 1]\n",
    "            x0 = self.step(x0, h, t) # [bs, 2]\n",
    "            tjs.append(x0.clone())\n",
    "        return torch.stack(tjs, dim=1) # [bs,nts,2]\n",
    "\n",
    "class ODE(ABC):\n",
    "    @abstractmethod \n",
    "    def drift_term(self, x, t):\n",
    "        pass \n",
    "\n",
    "class EulerSampler(Simulator):\n",
    "    def __init__(self, ode: ODE):\n",
    "        self.ode = ode \n",
    "    \n",
    "    def step(self, x, h, t):\n",
    "        return x + self.ode.drift_term(x, t) * h\n",
    "\n",
    "class SDE(ABC):\n",
    "    @abstractmethod\n",
    "    def drift_term(self, x, t):\n",
    "        pass \n",
    "    \n",
    "    @abstractmethod \n",
    "    def diff_term(self, x, t):\n",
    "        pass \n",
    "\n",
    "class EulerMarySampler(Simulator):\n",
    "    def __init__(self, sde: SDE):\n",
    "        self.sde = sde \n",
    "        \n",
    "    def step(self, x, h, t):\n",
    "        return x + self.sde.drift_term(x, t) * h + self.sde.diff_term(x, t) * torch.sqrt(h) * torch.randn_like(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(nn.Module, Density, Sampleable):\n",
    "    def __init__(self, mean, cov):\n",
    "        super().__init__()\n",
    "        self.mean = mean \n",
    "        self.cov = cov \n",
    "    \n",
    "    @property\n",
    "    def distribution(self):\n",
    "        return D.MultivariateNormal(self.mean, self.cov, validate_args=False)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.distribution.event_shape[0]\n",
    "\n",
    "    def sample(self, n_samples):\n",
    "        return self.distribution.sample((n_samples,))\n",
    "    \n",
    "    def log_density(self, x):\n",
    "        return self.distribution.log_prob(x)\n",
    "    \n",
    "    @classmethod \n",
    "    def isotropic(cls, dim: int, std: float):\n",
    "        mean = torch.zeros(dim)\n",
    "        cov = torch.eye(dim) * std ** 2 \n",
    "        return cls(mean, cov)\n",
    "\n",
    "class GaussianMixture(nn.Module, Density, Sampleable):\n",
    "    def __init__(self, mean, cov, weight):\n",
    "        super().__init__()\n",
    "        self.mean = mean \n",
    "        self.cov = cov \n",
    "        self.weight = weight\n",
    "\n",
    "    @property\n",
    "    def distribution(self):\n",
    "        return D.MixtureSameFamily(\n",
    "            mixture_distribution=D.Categorical(probs=self.weight, validate_args=False),\n",
    "            component_distribution=D.MultivariateNormal(self.mean, self.cov, validate_args=False),\n",
    "            validate_args=False\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.distribution.event_shape[0]\n",
    "\n",
    "    def log_density(self, x):\n",
    "        return self.distribution.log_prob(x)\n",
    "\n",
    "    def sample(self, n):\n",
    "        return self.distribution.sample((n,))\n",
    "\n",
    "    @classmethod \n",
    "    def random2d(cls, modes, std=1.0, scale=5.0, seed=0.0):\n",
    "        torch.manual_seed(seed)\n",
    "        mean = (torch.rand(modes, 2) - 0.5) * scale\n",
    "        cov = torch.diag_embed(torch.ones(modes, 2)) * std ** 2\n",
    "        weights = torch.ones(modes) / modes\n",
    "        return cls(mean, cov, weights)     \n",
    "\n",
    "    @classmethod \n",
    "    def symmetric2d(cls, modes, std=1.0, scale=5.0, seed=0.0):\n",
    "        torch.manual_seed(seed)\n",
    "        angles = torch.linspace(0, 2*torch.pi, modes + 1)[:modes]\n",
    "        mean = torch.stack([torch.cos(angles), torch.sin(angles)], dim=-1) * scale \n",
    "        cov = torch.diag_embed(torch.ones(modes, 2))\n",
    "        weights = torch.ones(modes) / modes\n",
    "        return cls(mean, cov, weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotting:\n",
    "    def __init__(self, sampler: Sampleable | Density, n_samples: Optional[int] = 10, ax: Optional[Axes] = None, **kwargs):\n",
    "        self.sampler = sampler \n",
    "        self.n_samples = n_samples\n",
    "        self.ax = ax if ax is not None else plt.gca()\n",
    "        self.kwargs = kwargs \n",
    "    \n",
    "    def scatter(self, x: torch.Tensor = None, y: torch.Tensor = None):\n",
    "        if x is None or y is None:\n",
    "            samples = self.sampler.sample(self.n_samples)\n",
    "            x, y = samples[:, 0], samples[:, 1]\n",
    "        self.ax.scatter(x, y, **self.kwargs)\n",
    "    \n",
    "    def plot_trajectory(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        self.ax.plot(x, y, **self.kwargs)\n",
    "\n",
    "\n",
    "    def hist(self):\n",
    "        samples = self.sampler.sample(self.n_samples)\n",
    "        for idx in range(samples.shape[-1]):\n",
    "            self.ax.hist(samples[:, idx], **self.kwargs)\n",
    "    \n",
    "    def hist2d(self):\n",
    "        samples = self.sampler.sample(self.n_samples)\n",
    "        x, y = samples[:, 0], samples[:, 1]\n",
    "        self.ax.hist2d(x, y, **self.kwargs)\n",
    "    \n",
    "    def get_density(self, scale, bins):\n",
    "        x = torch.linspace(-scale, scale, bins)\n",
    "        y = torch.linspace(-scale, scale, bins)\n",
    "        X, Y = torch.meshgrid(x, y) # make all possible pairs of x and y\n",
    "        xy = torch.stack([X.flatten(), Y.flatten()], dim=-1)\n",
    "        density = self.sampler.log_density(xy).view(bins, bins).T\n",
    "        return density\n",
    "\n",
    "    def imshow(self, scale: float, bins: int):\n",
    "        density = self.get_density(scale, bins)        \n",
    "        self.ax.imshow(\n",
    "            density, \n",
    "            extent=[-scale, scale] * 2,\n",
    "            origin='lower', \n",
    "            **self.kwargs\n",
    "        )\n",
    "    \n",
    "    def contour(self, scale, bins):\n",
    "        density = self.get_density(scale, bins)\n",
    "        self.ax.contour(\n",
    "            density, \n",
    "            extent=[-scale, scale] * 2,\n",
    "            origin='lower',\n",
    "            **self.kwargs\n",
    "        )\n",
    "    \n",
    "    def quiver(self, scale: float, bins: int, ts: torch.Tensor, score_model):\n",
    "        pairs = torch.meshgrid(torch.linspace(-scale, scale, bins), torch.linspace(-scale, scale, bins))\n",
    "        xx = pairs[0].reshape(-1, 1)\n",
    "        yy = pairs[1].reshape(-1, 1)\n",
    "        xy = torch.cat([xx, yy], dim=-1)\n",
    "        \n",
    "        t = ts.view(-1, 1).repeat(bins**2, 1)\n",
    "        score = score_model(xy, t)\n",
    "\n",
    "        self.ax.quiver(xy[:,0].detach(), xy[:,1].detach(), score[:,0].detach(), score[:,1].detach(), scale=125, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Conditional Probability Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GaussianMixture.random2d(modes=5, std=0.5, scale=30.0, seed=12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "Plotting(sampler=gs, n_samples=100,ax=ax, vmin=-15.0, cmap=plt.get_cmap('Blues')).imshow(scale=20.0, bins=200)\n",
    "Plotting(sampler=gs, n_samples=100,ax=ax, linestyles='solid', levels=20, colors='grey', alpha=0.25).contour(scale=20.0, bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"scale\": 15.0,\n",
    "    \"target_scale\": 10.0,\n",
    "    \"target_std\": 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = PARAMS['scale']\n",
    "fig, axes = plt.subplots(1,3, figsize=(24,8))\n",
    "p_simple = Gaussian.isotropic(dim=2, std = 1.0)\n",
    "p_data = GaussianMixture.symmetric2d(modes=5, std=PARAMS[\"target_std\"], scale=PARAMS[\"target_scale\"])\n",
    "\n",
    "Plotting(sampler=p_simple, ax=axes[0], vmin=-10, alpha=0.25, cmap=plt.get_cmap('Reds')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=p_data, ax=axes[1], vmin=-10, alpha=0.25, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)\n",
    "\n",
    "Plotting(sampler=p_simple, ax=axes[2], vmin=-10, alpha=0.25, cmap=plt.get_cmap('Reds')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=p_data, ax=axes[2], vmin=-10, alpha=0.25, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alpha(ABC):\n",
    "    def __init__(self):\n",
    "        assert torch.allclose(\n",
    "            self(torch.zeros(1,1)), torch.zeros(1,1)\n",
    "        )\n",
    "        assert torch.allclose(\n",
    "            self(torch.ones(1,1)), torch.ones(1,1)\n",
    "        )\n",
    "    @abstractmethod\n",
    "    def __call__(self, t):\n",
    "        pass \n",
    "    \n",
    "    def dt(self, t):\n",
    "        t = t.unsqueeze(-1) # the last dimension must be 1 for broadcasting \n",
    "        return vmap(jacrev(t)).view(-1, 1)\n",
    "\n",
    "class Beta(ABC):\n",
    "    def __init__(self):\n",
    "        assert torch.allclose(\n",
    "            self(torch.zeros(1,1)), torch.ones(1,1)\n",
    "        )\n",
    "        assert torch.allclose(\n",
    "            self(torch.ones(1,1)), torch.zeros(1,1)\n",
    "        )\n",
    "    @abstractmethod \n",
    "    def __call__(self, t):\n",
    "        pass \n",
    "    \n",
    "    def dt(self, t):\n",
    "        t = t.unsqueeze(-1)\n",
    "        return vmap(jacrev(self))(t).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAlpha(Alpha):\n",
    "    def __call__(self, t):\n",
    "        return t\n",
    "    \n",
    "    def dt(self, t):\n",
    "        return torch.ones_like(t)\n",
    "\n",
    "class LinearBeta(Beta):\n",
    "    def __call__(self, t):\n",
    "        return 1 - t\n",
    "\n",
    "    def dt(self, t):\n",
    "        return -torch.ones_like(t)\n",
    "\n",
    "class SqrtBeta(Beta):\n",
    "    def __call__(self, t):\n",
    "        return torch.sqrt(1 - t)\n",
    "    \n",
    "    def dt(self, t):\n",
    "        return - 0.5 / (torch.sqrt(1 - t) + 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPP(nn.Module, ABC):\n",
    "    def __init__(self, p_simple: Sampleable, p_data: Sampleable):\n",
    "        super().__init__()\n",
    "        self.p_simple = p_simple\n",
    "        self.p_data = p_data\n",
    "\n",
    "    def sample_marginal_path(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        sample randomly from the data distribution \n",
    "        sample from the path between data point z and starting point \n",
    "        \"\"\"\n",
    "        num_samples = t.shape[0]\n",
    "        z = self.sample_conditioning_variable(num_samples)  \n",
    "        x = self.sample_conditional_path(z, t) \n",
    "        return x\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample_conditioning_variable(self, num_samples: int) -> torch.Tensor: \n",
    "        \"\"\"\n",
    "        sample from the data distribution p(z)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def sample_conditional_path(self, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        sample from the path between data point z and initial distribution data point P(.|z)\n",
    "        \"\"\"\n",
    "        pass \n",
    "        \n",
    "    @abstractmethod\n",
    "    def conditional_vector_field(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        the path which is being followed by the conditional probability path u(x|z)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def conditional_score(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute: derivative(log(p(x|z)))\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCPP(CPP):\n",
    "    def __init__(self, p_data: Sampleable, alpha: LinearAlpha, beta: LinearBeta):\n",
    "        p_simple = Gaussian.isotropic(p_data.dim, 1.0)\n",
    "        super().__init__(p_simple, p_data)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "    def sample_conditioning_variable(self, n_sample: int) -> torch.Tensor:\n",
    "        return self.p_data.sample(n_sample)\n",
    "\n",
    "    def sample_conditional_path(self, z, t):\n",
    "        # sampling from standard gaussian randn() and then changing \n",
    "        # standard deviation and mean of that to represent sampling \n",
    "        # from isotropic gaussian \n",
    "        return self.alpha(t) * z + self.beta(t) * torch.randn_like(z)\n",
    "\n",
    "    def conditional_vector_field(self, x, z, t):\n",
    "        alpha_t = self.alpha(t)\n",
    "        alpha_dt = self.alpha.dt(t)\n",
    "        beta_t = self.beta(t)\n",
    "        beta_dt = self.beta.dt(t)\n",
    "        return (alpha_dt - beta_dt/beta_t * alpha_t) * z + (beta_dt / beta_t) * x\n",
    "\n",
    "    def conditional_score(self, x, z, t):\n",
    "        return (self.alpha(t) * z - x) / self.beta(t) ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_path = GCPP(\n",
    "    p_data=GaussianMixture.symmetric2d(modes=5, scale=20.0),\n",
    "    alpha=LinearAlpha(),\n",
    "    beta=SqrtBeta() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_samples = conditional_path.sample_conditioning_variable(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 30.0\n",
    "ax = plt.gca() \n",
    "\n",
    "Plotting(sampler=conditional_path.p_data, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=conditional_path.p_data, ax=ax, linestyles='solid', levels=20, colors='grey', alpha=0.25).contour(scale=scale, bins=200)\n",
    "\n",
    "ts = torch.linspace(0.0, 1.0, 7)\n",
    "z = conditional_path.sample_conditioning_variable(1)\n",
    "Plotting(sampler=conditional_path.p_data, ax=ax, marker='*', color='red').scatter(x=z[:,0], y=z[:,1])\n",
    "\n",
    "for t in ts:\n",
    "    samples = conditional_path.sample_conditional_path(z.repeat(10, 1), t.repeat(10, 1))\n",
    "    ax.scatter(samples[:, 0], samples[:, 1], alpha=0.5) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Vector Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVF(ODE):\n",
    "    def __init__(self, cpp: CPP, z: torch.Tensor):\n",
    "        self.cond_prob_path = cpp \n",
    "        self.z = z \n",
    "\n",
    "    def drift_term(self, x, t):\n",
    "        bs = x.shape[0]\n",
    "        z = self.z.expand(bs, *self.z.shape[1:])\n",
    "        return self.cond_prob_path.conditional_vector_field(x, z, t)\n",
    "\n",
    "class ConditionalVFStoc(SDE):\n",
    "    def __init__(self, cpp: CPP, z: torch.Tensor, sigma: float):\n",
    "        self.cpp = cpp \n",
    "        self.z = z\n",
    "        self.sigma = sigma \n",
    "\n",
    "    def drift_term(self, x, t):\n",
    "        z = self.z.expand(x.shape[0], *self.z.shape[1:])\n",
    "        return self.cpp.conditional_vector_field(x, z, t) + \\\n",
    "            0.5 * self.sigma ** 2 * self.cpp.conditional_score(x, z, t)\n",
    "\n",
    "    def diff_term(self, x, t):\n",
    "        return self.sigma * torch.ones_like(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_path = GCPP(\n",
    "    p_data=GaussianMixture.symmetric2d(modes=5, scale=20.0),\n",
    "    alpha=LinearAlpha(),\n",
    "    beta=SqrtBeta() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = conditional_path.sample_conditioning_variable(1)\n",
    "ode = ConditionalVF(cpp=conditional_path, z=z)\n",
    "sim = EulerSampler(ode=ode) \n",
    "\n",
    "ts = torch.linspace(0.0, 1.0, 100)\n",
    "x0 = conditional_path.sample_conditional_path(z.repeat(10, 1), ts[0].repeat(10, 1))\n",
    "tjs = sim.simulate_with_trajectory(x0, ts.view(-1, 1, 1).repeat(1, 10, 1)) #[bs, 2], [nts, bs, 1]\n",
    "z.shape, x0.shape, ts.shape, tjs.shape # [bs, nts, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 30.0\n",
    "ax = plt.gca() \n",
    "\n",
    "Plotting(sampler=conditional_path.p_data, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=conditional_path.p_simple, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Reds')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=conditional_path.p_data, ax=ax, linestyles='solid', levels=20, colors='grey', alpha=0.25).contour(scale=scale, bins=200)\n",
    "\n",
    "ax.scatter(z[:,0], z[:, 1], marker='*', color='red')\n",
    "ax.scatter(x0[:, 0], x0[:, 1], alpha=0.5)\n",
    "\n",
    "for idx in range(tjs.shape[0]):\n",
    "    ax.plot(tjs[idx, :, 0], tjs[idx, :, 1], color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.88\n",
    "z = conditional_path.sample_conditioning_variable(1)\n",
    "sde = ConditionalVFStoc(cpp=conditional_path, z=z, sigma=sigma)\n",
    "sim = EulerMarySampler(sde=sde)\n",
    "\n",
    "ts = torch.linspace(0.0, 1.0, 100)\n",
    "x0 = conditional_path.sample_conditional_path(z.repeat(10, 1), ts[0].repeat(10, 1))\n",
    "tjs = sim.simulate_with_trajectory(x0, ts.view(-1, 1, 1).repeat(1, 10, 1)) #[bs, 2], [nts, bs, 1]\n",
    "z.shape, x0.shape, ts.shape, tjs.shape # [bs, nts, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 30.0\n",
    "ax = plt.gca() \n",
    "\n",
    "Plotting(sampler=conditional_path.p_data, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=conditional_path.p_simple, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Reds')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=conditional_path.p_data, ax=ax, linestyles='solid', levels=20, colors='grey', alpha=0.25).contour(scale=scale, bins=200)\n",
    "\n",
    "ax.scatter(z[:,0], z[:, 1], marker='*', color='red')\n",
    "ax.scatter(x0[:, 0], x0[:, 1], alpha=0.5)\n",
    "\n",
    "for idx in range(tjs.shape[0]):\n",
    "    ax.plot(tjs[idx, :, 0], tjs[idx, :, 1], color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(dims: List[int], activation: Type[nn.Module] = nn.SiLU):\n",
    "    mlp = [] \n",
    "    for idx in range(len(dims) - 1):\n",
    "        mlp.append(nn.Linear(dims[idx], dims[idx + 1]))\n",
    "        if idx < len(dims) - 2:\n",
    "            mlp.append(activation())\n",
    "    return nn.Sequential(*mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPVF(nn.Module):\n",
    "    def __init__(self, dim: int, hiddens: List[int]):\n",
    "        super().__init__()\n",
    "        self.dim = dim \n",
    "        self.net = build_mlp([dim + 1]+hiddens+[dim])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        xt = torch.cat([x, t], dim=-1) #[bs, dim], [nts, bs, 1] = [nts, bs, dim+1]\n",
    "        return self.net(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(ABC):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model \n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_train_loss(self, **kwargs) -> torch.Tensor:\n",
    "        pass \n",
    "\n",
    "    def get_optimizer(self, lr: float):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, n_epochs: int, device: torch.device, lr:float=1e-3, **kwargs):\n",
    "        self.model.to(device)\n",
    "        opt = self.get_optimizer(lr)\n",
    "        self.model.train()\n",
    "\n",
    "        pbar = tqdm(enumerate(range(n_epochs)))\n",
    "        for idx, ep in pbar:\n",
    "            opt.zero_grad()\n",
    "            loss = self.get_train_loss(**kwargs)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if idx%1000 == 0:\n",
    "                print() \n",
    "            pbar.set_description(f'Epoch {idx}, loss: {loss.item()}')\n",
    "    \n",
    "        self.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFMTrainer(Trainer):\n",
    "    def __init__(self, cond_path: CPP, model: MLPVF, **kwargs):\n",
    "        super().__init__(model, **kwargs)\n",
    "        self.cond_path = cond_path \n",
    "    \n",
    "    def get_train_loss(self, batch_size: int):\n",
    "        z = self.cond_path.p_data.sample(batch_size) # [bs, 1]\n",
    "        t = torch.rand(batch_size, 1).to(z) # [bs, 1]\n",
    "        x = self.cond_path.sample_conditional_path(z, t) # [bs, 2]\n",
    "\n",
    "        u_theta = self.model(x, t)\n",
    "        u_ref = self.cond_path.conditional_vector_field(x, z, t)\n",
    "        error = torch.sum(torch.square(u_theta - u_ref), dim=-1)\n",
    "        return torch.mean(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"scale\": 15.0,\n",
    "    \"target_scale\": 10.0,\n",
    "    \"target_std\": 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_path = GCPP(\n",
    "    p_data=GaussianMixture.symmetric2d(\n",
    "        modes=5, \n",
    "        std=PARAMS['target_std'],\n",
    "        scale=PARAMS['target_scale']\n",
    "    ),\n",
    "    alpha=LinearAlpha(),\n",
    "    beta=SqrtBeta()\n",
    ") \n",
    "mlp = MLPVF(dim=2, hiddens=[64,64,64,64])\n",
    "trainer = CFMTrainer(cond_path=cond_path, model=mlp)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(n_epochs=5000, device=device, lr=1e-3, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedVF(ODE):\n",
    "    def __init__(self, net: MLPVF):\n",
    "        self.net = net \n",
    "    \n",
    "    def drift_term(self, x, t):\n",
    "        return self.net(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 20\n",
    "fig, axes = plt.subplots(1,4, figsize=(36, 12))\n",
    "\n",
    "ax = axes[3]\n",
    "ax.set_title('MLP Marginal VF', fontsize=20)\n",
    "Plotting(sampler=cond_path.p_data, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=cond_path.p_simple, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Reds')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=cond_path.p_data, ax=ax, linestyles='solid', levels=20, colors='grey', alpha=0.25).contour(scale=scale, bins=200)\n",
    "\n",
    "bs = 100\n",
    "\n",
    "ts = torch.linspace(0.0, 1.0, bs).view(-1, 1, 1).repeat(1, bs, 1) # [nts, bs, 1]\n",
    "\n",
    "ode = LearnedVF(net=mlp)\n",
    "sim = EulerSampler(ode=ode)\n",
    "x0 = cond_path.p_simple.sample(bs)\n",
    "tjs = sim.simulate_with_trajectory(x0, ts) # [bs, 2], [nts, bs, 1] = [bs, nts, 2]\n",
    "\n",
    "for bs in range(tjs.shape[0]):\n",
    "    Plotting(sampler=cond_path, ax=ax, color='black', alpha=0.25).plot_trajectory(tjs[bs, :, 0].detach(), tjs[bs, :, 1].detach())\n",
    "    \n",
    "\n",
    "ax = axes[2]\n",
    "ax.set_title('MLP learned VF', fontsize=20)\n",
    "Plotting(sampler=cond_path.p_data, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=cond_path.p_simple, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Reds')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=cond_path.p_data, ax=ax, linestyles='solid', levels=20, colors='grey', alpha=0.25).contour(scale=scale, bins=200)\n",
    "\n",
    "bs = 100\n",
    "ts = torch.linspace(0.0, 1.0, 7).view(-1, 1, 1).repeat(1, bs, 1)\n",
    "x0 = cond_path.p_simple.sample(bs)\n",
    "\n",
    "ode = LearnedVF(net=mlp)\n",
    "sim = EulerSampler(ode=ode)\n",
    "xts = sim.simulate_with_trajectory(x0, ts)\n",
    "\n",
    "for nts in range(xts.shape[1]):\n",
    "    ax.scatter(xts[:, nts, 0], xts[:, nts, 1])\n",
    "    \n",
    "    \n",
    "ax = axes[0]\n",
    "ax.set_title('ground truth marginal VF', fontsize=20)\n",
    "Plotting(sampler=cond_path.p_data, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=cond_path.p_simple, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Reds')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=cond_path.p_data, ax=ax, linestyles='solid', levels=20, colors='grey', alpha=0.25).contour(scale=scale, bins=200)\n",
    "\n",
    "bs = 100\n",
    "ts = torch.linspace(0.0, 1.0, 7).view(-1, 1, 1).repeat(1, bs, 1) # [nts, bs, 1]\n",
    "x0 = cond_path.p_simple.sample(bs)\n",
    "\n",
    "for idx in range(ts.shape[0]):\n",
    "    samples = cond_path.sample_marginal_path(ts[idx])\n",
    "    ax.scatter(samples[:, 0], samples[:, 1])\n",
    "    \n",
    "    \n",
    "    \n",
    "ax = axes[1]\n",
    "ax.set_title('Ground Truth Marginal VF', fontsize=20)\n",
    "Plotting(sampler=cond_path.p_data, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=cond_path.p_simple, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Reds')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=cond_path.p_data, ax=ax, linestyles='solid', levels=20, colors='grey', alpha=0.25).contour(scale=scale, bins=200)\n",
    "\n",
    "bs = 100\n",
    "ts = torch.linspace(0.0, 1.0, bs).view(-1, 1, 1).repeat(1, bs, 1) # [nts, bs, 1]\n",
    "\n",
    "z = cond_path.sample_conditioning_variable(bs)\n",
    "ode = ConditionalVF(cpp=cond_path, z=z)\n",
    "\n",
    "sim = EulerSampler(ode=ode)\n",
    "x0 = cond_path.p_simple.sample(bs)\n",
    "tjs = sim.simulate_with_trajectory(x0, ts) # [bs, 2], [nts, bs, 1] = [bs, nts, 2]\n",
    "\n",
    "for bs in range(tjs.shape[0]):\n",
    "    Plotting(sampler=cond_path, ax=ax, color='black', alpha=0.25).plot_trajectory(tjs[bs, :, 0].detach(), tjs[bs, :, 1].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSMTrainer(Trainer):\n",
    "    def __init__(self, cond_path: CPP, model: MLPVF, **kwargs):\n",
    "        super().__init__(model, **kwargs)\n",
    "        self.cond_path = cond_path \n",
    "    \n",
    "    def get_train_loss(self, batch_size: int):\n",
    "        z = self.cond_path.p_data.sample(batch_size) # [bs, 1]\n",
    "        t = torch.rand(batch_size, 1).to(z) # [bs, 1]\n",
    "        x = self.cond_path.sample_conditional_path(z, t) # [bs, 2]\n",
    "\n",
    "        s_theta = self.model(x, t)\n",
    "        s_ref = self.cond_path.conditional_score(x, z, t)\n",
    "        error = torch.sum(torch.square(s_theta - s_ref), dim=-1)\n",
    "        return torch.mean(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"scale\": 20.0,\n",
    "    \"target_scale\": 15.0,\n",
    "    \"target_std\": 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = GCPP(\n",
    "    p_data=GaussianMixture.symmetric2d(\n",
    "        modes=5, \n",
    "        std=PARAMS['target_std'],\n",
    "        scale=PARAMS['target_scale']\n",
    "    ),\n",
    "    alpha=LinearAlpha(),\n",
    "    beta=SqrtBeta()\n",
    ") \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "flow_model = MLPVF(dim=2, hiddens=[64,64,64,64])\n",
    "score_model = MLPVF(dim=2, hiddens=[64,64,64,64])\n",
    "\n",
    "flow_trainer = CFMTrainer(cond_path=path, model=flow_model)\n",
    "score_trainer = CSMTrainer(cond_path=path, model=score_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training flow model')\n",
    "flow_trainer.train(n_epochs=5000, device=device, lr=1e-3, batch_size=1000)\n",
    "print('training score model')\n",
    "score_trainer.train(n_epochs=5000, device=device, lr=1e-3, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class langevinSDE(SDE):\n",
    "    def __init__(self, flow_model: MLPVF, score_model: MLPVF, sigma: float):\n",
    "        self.flow_model = flow_model \n",
    "        self.score_model = score_model \n",
    "        self.sigma = sigma \n",
    "    \n",
    "    def drift_term(self, x, t):\n",
    "        return self.flow_model(x, t) + 0.5 * self.sigma ** 2 * self.score_model(x, t)\n",
    "        \n",
    "    def diff_term(self, x, t):\n",
    "        return self.sigma * torch.randn_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = PARAMS['scale'] \n",
    "sigma = 0.6\n",
    "fig, axes = plt.subplots(1,4, figsize=(36, 12))\n",
    "\n",
    "############################################\n",
    "ax = axes[3]\n",
    "ax.set_title('MLP Marginal VF', fontsize=20)\n",
    "Plotting(sampler=path.p_data, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=path.p_simple, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Reds')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=path.p_data, ax=ax, linestyles='solid', levels=20, colors='grey', alpha=0.25).contour(scale=scale, bins=200)\n",
    "\n",
    "bs = 100\n",
    "\n",
    "ts = torch.linspace(0.0, 1.0, bs).view(-1, 1, 1).repeat(1, bs, 1) # [nts, bs, 1]\n",
    "\n",
    "sde = langevinSDE(flow_model=flow_model, score_model=score_model, sigma=sigma)\n",
    "sim = EulerMarySampler(sde=sde)\n",
    "x0 = path.p_simple.sample(bs)\n",
    "tjs = sim.simulate_with_trajectory(x0, ts) # [bs, 2], [nts, bs, 1] = [bs, nts, 2]\n",
    "\n",
    "for bs in range(tjs.shape[0]):\n",
    "    Plotting(sampler=path, ax=ax, color='black', alpha=0.25).plot_trajectory(tjs[bs, :, 0].detach(), tjs[bs, :, 1].detach())\n",
    "    \n",
    "############################################\n",
    "ax = axes[2]\n",
    "ax.set_title('MLP learned VF', fontsize=20)\n",
    "Plotting(sampler=path.p_data, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=path.p_simple, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Reds')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=path.p_data, ax=ax, linestyles='solid', levels=20, colors='grey', alpha=0.25).contour(scale=scale, bins=200)\n",
    "\n",
    "bs = 100\n",
    "ts = torch.linspace(0.0, 1.0, 7).view(-1, 1, 1).repeat(1, bs, 1)\n",
    "x0 = path.p_simple.sample(bs)\n",
    "\n",
    "sde = langevinSDE(flow_model=flow_model, score_model=score_model, sigma=sigma)\n",
    "sim = EulerMarySampler(sde=sde)\n",
    "xts = sim.simulate_with_trajectory(x0, ts)\n",
    "\n",
    "for nts in range(xts.shape[1]):\n",
    "    ax.scatter(xts[:, nts, 0], xts[:, nts, 1])\n",
    "    \n",
    "############################################\n",
    "ax = axes[0]\n",
    "ax.set_title('ground truth marginal VF', fontsize=20)\n",
    "Plotting(sampler=path.p_data, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=path.p_simple, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Reds')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=path.p_data, ax=ax, linestyles='solid', levels=20, colors='grey', alpha=0.25).contour(scale=scale, bins=200)\n",
    "\n",
    "bs = 100\n",
    "ts = torch.linspace(0.0, 1.0, 7).view(-1, 1, 1).repeat(1, bs, 1) # [nts, bs, 1]\n",
    "x0 = path.p_simple.sample(bs)\n",
    "\n",
    "for idx in range(ts.shape[0]):\n",
    "    samples = path.sample_marginal_path(ts[idx])\n",
    "    ax.scatter(samples[:, 0], samples[:, 1])\n",
    "    \n",
    "    \n",
    "############################################\n",
    "ax = axes[1]\n",
    "ax.set_title('Ground Truth Marginal VF', fontsize=20)\n",
    "Plotting(sampler=path.p_data, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=path.p_simple, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Reds')).imshow(scale=scale, bins=200)\n",
    "Plotting(sampler=path.p_data, ax=ax, linestyles='solid', levels=20, colors='grey', alpha=0.25).contour(scale=scale, bins=200)\n",
    "\n",
    "bs = 100\n",
    "ts = torch.linspace(0.0, 1.0, bs).view(-1, 1, 1).repeat(1, bs, 1) # [nts, bs, 1]\n",
    "\n",
    "z = path.sample_conditioning_variable(bs)\n",
    "ode = ConditionalVF(cpp=path, z=z)\n",
    "\n",
    "sim = EulerSampler(ode=ode)\n",
    "x0 = path.p_simple.sample(bs)\n",
    "tjs = sim.simulate_with_trajectory(x0, ts) # [bs, 2], [nts, bs, 1] = [bs, nts, 2]\n",
    "xts = path.sample_marginal_path(ts)\n",
    "\n",
    "# for bs in range(xts.shape[1]):\n",
    "#     Plotting(sampler=path, ax=ax, color='black', alpha=0.25).plot_trajectory(xts[:, bs, 0].detach(), xts[:, bs, 1].detach())\n",
    "\n",
    "for bs in range(tjs.shape[0]):\n",
    "    Plotting(sampler=path, ax=ax, color='black', alpha=0.25).plot_trajectory(tjs[bs, :, 0].detach(), tjs[bs, :, 1].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca() \n",
    "xts = path.sample_marginal_path(ts)\n",
    "\n",
    "for bs in range(xts.shape[1]):\n",
    "    Plotting(sampler=path, ax=ax, color='black', alpha=0.25).plot_trajectory(xts[:, bs, 0].detach(), xts[:, bs, 1].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization the score vector field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow_model to predict the vector field u_t(x)\n",
    "# score_model to predict the score dt.log.p(x)\n",
    "# score_vf_model to predict the score vf -> for visualization of the score vector field "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tilde{s}_t^{\\theta}(x) = \\frac{u_t^{\\theta}(x) - a_tx}{b_t} = \\frac{\\alpha_t u_t^{\\theta}(x) - \\dot{\\alpha}_t x}{\\beta_t^2 \\dot{\\alpha}_t - \\alpha_t \\dot{\\beta}_t \\beta_t},$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreVF(nn.Module):\n",
    "    def __init__(self, flow_model: MLPVF, alpha: Alpha, beta: Beta):\n",
    "        super().__init__()\n",
    "        self.flow_model = flow_model\n",
    "        self.alpha = alpha \n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        alpha_t = self.alpha(t)\n",
    "        alpha_dt = self.alpha.dt(t)\n",
    "        beta_t = self.beta(t)\n",
    "        beta_dt = self.beta.dt(t)\n",
    "        \n",
    "        num = alpha_t * self.flow_model(x, t) - alpha_dt * x \n",
    "        den = beta_t ** 2 * alpha_dt - alpha_t * beta_dt * beta_t\n",
    "        \n",
    "        return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 30\n",
    "num_marginals = 4\n",
    "\n",
    "learned_score_model = score_model\n",
    "flow_score_model = ScoreVF(flow_model, path.alpha, path.beta)\n",
    "\n",
    "fig, axes = plt.subplots(2, num_marginals, figsize=(6 * num_marginals, 12))\n",
    "\n",
    "scale = PARAMS[\"scale\"]\n",
    "\n",
    "ts = torch.linspace(0.0, 0.9999, num_marginals).to(device)\n",
    "\n",
    "axes[0,0].set_ylabel(\"Learned with Score Matching\", fontsize=12)\n",
    "axes[1,0].set_ylabel(\"Computed from $u_t^{{\\\\theta}}(x)$\", fontsize=12)\n",
    "\n",
    "for idx in range(num_marginals):\n",
    "    t = ts[idx]\n",
    "    \n",
    "    # Learned scores\n",
    "    ax = axes[0, idx]\n",
    "    Plotting(sampler=path.p_simple, ax=ax).quiver(scale=scale, bins=num_bins, ts=t, score_model=learned_score_model) \n",
    "    \n",
    "    Plotting(sampler=path.p_simple, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Reds')).imshow(scale=scale, bins=200)\n",
    "    Plotting(sampler=path.p_data, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)\n",
    "    \n",
    "    # Flow score model\n",
    "    ax = axes[1, idx]\n",
    "    Plotting(sampler=path.p_simple, ax=ax).quiver(scale=scale, bins=num_bins, ts=t, score_model=flow_score_model) \n",
    "    \n",
    "    Plotting(sampler=path.p_simple, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Reds')).imshow(scale=scale, bins=200)\n",
    "    Plotting(sampler=path.p_data, ax=ax, vmin=-10, alpha=0.75, cmap=plt.get_cmap('Blues')).imshow(scale=scale, bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow matching between arbitrary distribution and linear conditional probability path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoonSampleable(Sampleable):\n",
    "    def __init__(self, noise: float=0.05, scale: float=5.0, offset: Optional[torch.Tensor]=None):\n",
    "        self.noise = noise \n",
    "        self.scale = scale\n",
    "        if offset is None:\n",
    "            offset = torch.zeros(2)\n",
    "        self.offset = offset\n",
    "\n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return 2\n",
    "    \n",
    "    def sample(self, n_samples: int) -> torch.Tensor:\n",
    "        samples, _ = make_moons(\n",
    "            n_samples=n_samples,\n",
    "            noise=self.noise,\n",
    "            random_state=None\n",
    "        )\n",
    "        return self.scale * torch.from_numpy(samples.astype(np.float32))\n",
    "\n",
    "class CircleSampleable(Sampleable):\n",
    "    def __init__(self, noise: float=0.05, scale: float=5.0, offset: Optional[torch.Tensor] = None):\n",
    "        self.noise = noise \n",
    "        self.scale = scale \n",
    "        if offset is None:\n",
    "            offset = torch.zeros(2)\n",
    "        self.offset = offset \n",
    "    @property \n",
    "    def dim(self) -> int:\n",
    "        return 2\n",
    "        \n",
    "    def sample(self, n_samples: int) -> torch.Tensor:\n",
    "        samples, _ = make_circles(\n",
    "            n_samples=n_samples,\n",
    "            noise=self.noise,\n",
    "            factor=0.5,\n",
    "            random_state=None\n",
    "        )\n",
    "        return self.scale * torch.from_numpy(samples.astype(np.float32))\n",
    "        \n",
    "\n",
    "class CheckerboardSampleable(Sampleable):\n",
    "    def __init__(self, grid_size: int=3, scale: float=5.0):\n",
    "        self.grid_size = grid_size\n",
    "        self.scale = scale\n",
    "\n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return 2\n",
    "\n",
    "    def sample(self, num_samples: int) -> torch.Tensor:\n",
    "  \n",
    "        grid_length = 2 * self.scale / self.grid_size\n",
    "        samples = torch.zeros(0,2)\n",
    "        \n",
    "        while samples.shape[0] < num_samples:\n",
    "            new_samples = (torch.rand(num_samples,2) - 0.5) * 2 * self.scale\n",
    "            x_mask = torch.floor((new_samples[:,0] + self.scale) / grid_length) % 2 == 0 # (bs,)\n",
    "            y_mask = torch.floor((new_samples[:,1] + self.scale) / grid_length) % 2 == 0 # (bs,)\n",
    "            accept_mask = torch.logical_xor(~x_mask, y_mask)\n",
    "            samples = torch.cat([samples, new_samples[accept_mask]], dim=0)\n",
    "        \n",
    "        return samples[:num_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotting(sampler=CircleSampleable(), n_samples=20000, bins=100).hist2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCPP(CPP):\n",
    "    def __init__(self, p_simple: Sampleable, p_data: Sampleable):\n",
    "        super().__init__(p_simple, p_data)\n",
    "\n",
    "    def sample_conditioning_variable(self, num_samples: int) -> torch.Tensor:\n",
    "        return self.p_data.sample(num_samples)\n",
    "    \n",
    "    def sample_conditional_path(self, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        # interpolant or p(.|z)\n",
    "        x0 = self.p_simple.sample(z.shape[0])\n",
    "        return (1 - t) * x0 + t * z\n",
    "        \n",
    "    def conditional_vector_field(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        return (z - x) / (1 - t)\n",
    "\n",
    "    def conditional_score(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        raise Exception(\"You should not be calling this function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = LCPP(\n",
    "    p_simple = Gaussian.isotropic(dim=2, std=1.0),\n",
    "    p_data = CheckerboardSampleable(grid_size=4)\n",
    ")\n",
    "z = path.p_data.sample(1) # (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist2d_samples(samples, ax: Optional[Axes] = None, bins: int = 200, scale: float = 5.0, percentile: int = 99, xrange=None, yrange=None, **kwargs):\n",
    "    if xrange is None:\n",
    "        xrange = [-scale, scale]\n",
    "    if yrange is None:\n",
    "        yrange = [-scale, scale]\n",
    "\n",
    "    H, xedges, yedges = np.histogram2d(samples[:, 0], samples[:, 1], bins=bins, range=[xrange, yrange])\n",
    "    \n",
    "    # Determine color normalization based on the 99th percentile\n",
    "    cmax = np.percentile(H, percentile)\n",
    "    cmin = 0.0\n",
    "    norm = cm.colors.Normalize(vmax=cmax, vmin=cmin)\n",
    "    \n",
    "    # Plot using imshow for more control\n",
    "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "    ax.imshow(H.T, extent=extent, origin='lower', norm=norm, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.linspace(0, 1, 5)\n",
    "n_marginals = 5\n",
    "_, axes = plt.subplots(3, 5, figsize=(6 * n_marginals, 6 * 3))\n",
    "scale = 6.0 \n",
    "n_samples = 10000\n",
    "\n",
    "for idx, t in enumerate(ts):\n",
    "    zz = z.repeat(n_samples, 1)\n",
    "    tt = t.view(1, 1).repeat(n_samples, 1)\n",
    "    percentile = min(99 + 2 * torch.sin(t).item(), 100)\n",
    "    samples = path.sample_conditional_path(zz, tt) # [100k, 2]\n",
    "    hist2d_samples(samples=samples.cpu(), ax=axes[0, idx], bins=300, scale=scale, percentile=percentile, alpha=1.0)\n",
    "    \n",
    "\n",
    "ode = ConditionalVF(path, z)\n",
    "sim = EulerSampler(ode=ode)\n",
    "x0 = path.p_simple.sample(n_samples)\n",
    "ts = torch.linspace(0, 1, 500)\n",
    "xts = sim.simulate_with_trajectory(x0, ts.view(-1,1,1).expand(-1,n_samples,1)) # [bs, nts, 2]\n",
    "# record_every_idxs = record_every(len(ts), len(ts) // (n_marginals - 1))\n",
    "\n",
    "# xts = xts[:,record_every_idxs,:] # [bs, nts//n, 2]\n",
    "\n",
    "for idx, val in enumerate([0, 100, 220, 350, 499]):\n",
    "    xx = xts[:,val,:]\n",
    "    tt = ts[val]\n",
    "    percentile = min(99 + 2 * torch.sin(tt).item(), 100)\n",
    "    hist2d_samples(samples=xx.cpu(), ax=axes[1, idx], bins=300, scale=scale, percentile=percentile, alpha=1.0)\n",
    "\n",
    "ts = torch.linspace(0.0, 1.0, n_marginals)\n",
    "for idx, t in enumerate(ts):\n",
    "    zz = z.expand(n_samples, -1)\n",
    "    tt = t.view(1,1).expand(n_samples,1)\n",
    "    xts = path.sample_marginal_path(tt)\n",
    "    hist2d_samples(samples=xts.cpu(), ax=axes[2, idx], bins=300, scale=scale, percentile=99, alpha=1.0)\n",
    "\n",
    "axes[0,idx].scatter(z[:,0], z[:,1], marker='*', color='red')\n",
    "axes[1,idx].scatter(z[:,0], z[:,1], marker='*', color='red')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between Gaussian and Linear Conditional Probability Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing gaussian conditional probability path \n",
    "gs_path = GCPP(    \n",
    "    p_data=GaussianMixture.symmetric2d(modes=5, std=PARAMS['target_std'], scale=PARAMS['target_scale']),\n",
    "    alpha=LinearAlpha(),\n",
    "    beta=SqrtBeta()\n",
    ")\n",
    "linear_path = LCPP(\n",
    "    p_simple = Gaussian.isotropic(dim=2, std=1.0),\n",
    "    p_data = CheckerboardSampleable(grid_size=4)\n",
    ")\n",
    "\n",
    "gs_z = gs_path.sample_conditioning_variable(1)\n",
    "ln_z = linear_path.sample_conditioning_variable(1) \n",
    "gs_z, ln_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the linear and guassian conditional probability paths\n",
    "Found out both of them are linear, over time the variance of the samples decreases and ultimates reaches the target distribution data point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.linspace(0, 1, 5)\n",
    "_, axes = plt.subplots(2, 5, figsize=(6 * 5, 6 * 2))\n",
    "\n",
    "for idx, t in enumerate(ts):\n",
    "    gs_samples = gs_path.sample_conditional_path(gs_z.repeat(n_samples, 1), t.view(1,1).repeat(n_samples, 1))\n",
    "    ln_samples = linear_path.sample_conditional_path(ln_z.repeat(n_samples, 1), t.view(1,1).repeat(n_samples, 1))\n",
    "    \n",
    "    percentile = min(99 + 2 * torch.sin(t).item(), 100)\n",
    "    hist2d_samples(samples=gs_samples.cpu(), ax=axes[0, idx], bins=300, scale=15, percentile=percentile, alpha=1.0)#, xrange=[-12,-9], yrange=[7, 10])\n",
    "    hist2d_samples(samples=ln_samples.cpu(), ax=axes[1, idx], bins=300, scale=6, percentile=percentile, alpha=1.0)#, xrange=[2,6],yrange=[-3,1])\n",
    "\n",
    "axes[0, idx].scatter(gs_z[:, 0], gs_z[:, 1], marker='*', color='red')\n",
    "axes[1, idx].scatter(ln_z[:, 0], ln_z[:, 1], marker='*', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if the vector field also takes to the target distribution data point \n",
    "\n",
    "If we follow the vector field, then we also reaches the target distribution data point exactly in the same manner as the conditional probability path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(ode, n_samples, nts):\n",
    "    sim = EulerSampler(ode=ode)\n",
    "    x0 = path.p_simple.sample(n_samples)\n",
    "    ts = torch.linspace(0, 1, nts)\n",
    "    return sim.simulate_with_trajectory(x0, ts.view(-1,1,1).expand(-1,n_samples,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(2, 5, figsize=(6 * 5, 6 * 2))\n",
    "\n",
    "ln_xts = simulate(ode=ConditionalVF(linear_path, ln_z), n_samples=n_samples, nts=500)\n",
    "gs_xts = simulate(ode=ConditionalVF(gs_path, gs_z), n_samples=n_samples, nts=500)\n",
    "\n",
    "ts = torch.linspace(0, 1, 500)\n",
    "\n",
    "for idx, t in enumerate([0, 100, 220, 350, 499]):\n",
    "    xts_1 = gs_xts[:, t, :]\n",
    "    xts_2 = ln_xts[:, t, :]\n",
    "    t = ts[t]\n",
    "            \n",
    "    percentile = min(99 + 2 * torch.sin(t).item(), 100)\n",
    "    hist2d_samples(samples=xts_1.cpu(), ax=axes[0, idx], bins=300, scale=15, percentile=percentile, alpha=1.0)\n",
    "    hist2d_samples(samples=xts_2.cpu(), ax=axes[1, idx], bins=300, scale=15, percentile=percentile, alpha=1.0)\n",
    "\n",
    "axes[0, idx].scatter(gs_z[:, 0], gs_z[:, 1], marker='*', color='red')\n",
    "axes[1, idx].scatter(ln_z[:, 0], ln_z[:, 1], marker='*', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if the marginal distribution takes us to the data distribution or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.linspace(0.0, 1.0, 5)\n",
    "_, axes = plt.subplots(2, 5, figsize=(6 * 5, 6 * 2))\n",
    "\n",
    "for idx, t in enumerate(ts):\n",
    "    ln_xts = linear_path.sample_marginal_path(t.view(1,1).expand(n_samples,1))\n",
    "    gs_xts = gs_path.sample_marginal_path(t.view(1,1).expand(n_samples,1))\n",
    "    \n",
    "    hist2d_samples(samples=gs_xts.cpu(), ax=axes[0, idx], bins=300, scale=20, percentile=99, alpha=1.0)\n",
    "    hist2d_samples(samples=ln_xts.cpu(), ax=axes[1, idx], bins=300, scale=15, percentile=99, alpha=1.0)\n",
    "\n",
    "axes[0, idx].scatter(gs_z[:, 0], gs_z[:, 1], marker='*', color='red')\n",
    "axes[1, idx].scatter(ln_z[:, 0], ln_z[:, 1], marker='*', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Flow Model to learn vector field from arbitrary source to target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_path = LCPP(\n",
    "    # p_simple = Gaussian.isotropic(dim=2, std=1.0),\n",
    "    p_simple = CircleSampleable(),\n",
    "    p_data = CheckerboardSampleable(grid_size=4)\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "flow_model = MLPVF(dim=2, hiddens=[64,64,64,64])\n",
    "\n",
    "flow_trainer = CFMTrainer(cond_path=linear_path, model=flow_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_trainer.train(n_epochs=20000, device=device, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(2, 5, figsize=(6 * 5, 6*2))\n",
    "nts = 1000\n",
    "\n",
    "ts = torch.linspace(0, 1, nts)\n",
    "x0 = linear_path.p_simple.sample(n_samples)\n",
    "ode = LearnedVF(flow_model)\n",
    "sim = EulerSampler(ode=ode)\n",
    "ln_xts = sim.simulate_with_trajectory(x0, ts.view(-1, 1, 1).repeat(1, n_samples, 1))\n",
    "\n",
    "for idx, t in enumerate([0, 399, 599, 799, 999]):\n",
    "    xts_t = ln_xts[:, t, :]\n",
    "    t = ts[t]\n",
    "    xts_gt = linear_path.sample_marginal_path(t.view(1,1).repeat(xts_t.shape[0], 1))\n",
    "            \n",
    "    hist2d_samples(samples=xts_t.cpu(), ax=axes[0, idx], bins=200, scale=6, percentile=99, alpha=1.0)\n",
    "    hist2d_samples(samples=xts_gt.cpu(), ax=axes[1, idx], bins=200, scale=6, percentile=99, alpha=1.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
